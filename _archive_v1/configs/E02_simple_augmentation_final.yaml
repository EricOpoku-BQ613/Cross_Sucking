# E02: Simple Augmentation (Pig Paper Inspired) - FINALIZED
# ==========================================================
# Based on Guo et al. 2026: +17% improvement with flip/rotate
# V3 Analysis: Heavy aug caused FPs and poor calibration (ECE=0.193)
# 
# Key changes from V3:
#   1. Simpler augmentation (geometric only, minimal color)
#   2. Temperature scaling post-hoc
#   3. Better overfitting monitoring
#   4. Slightly higher dropout (0.4 vs 0.3)
# ==========================================================

experiment:
  name: "E02_simple_augmentation"
  seed: 42
  description: "Simple aug + temperature scaling, fixing V3 calibration issues"

# -----------------------------
# DATA CONFIGURATION
# -----------------------------
data:
  train_manifest: "data/manifests/train_intravideo_20260105_162028.csv"
  val_manifest: "data/manifests/val_intravideo_20260105_162028.csv"
  
  # Video sampling
  clip_len: 16
  frame_rate: 12
  crop_size: 224
  
  # Class balancing (keep from V3 - worked well)
  balanced_sampling: true
  
  # Batching
  batch_size: 8
  num_workers: 8
  pin_memory: true
  prefetch_factor: 2

# -----------------------------
# AUGMENTATION - SIMPLE ONLY
# -----------------------------
augmentation:
  enabled: true
  use_temporal_consistent: true  # Same transform across all frames
  
  # === GEOMETRIC (main augmentation) ===
  p_horizontal_flip: 0.5
  p_vertical_flip: 0.5          # Overhead camera - vertical flip is valid
  max_rotation: 15.0            # Degrees
  crop_scale_min: 0.85          # Random resized crop
  crop_scale_max: 1.0
  max_translate: 0.05           # 5% max translation
  
  # === COLOR - MINIMAL ===
  # Reduced from V3 to improve calibration
  brightness: 0.15              # V3 had 0.3
  contrast: 0.15                # V3 had 0.3
  saturation: 0.05              # V3 had 0.2
  hue: 0.01                     # V3 had 0.05
  
  # === DISABLED (were causing FPs in V3) ===
  p_grayscale: 0.0
  p_gaussian_blur: 0.0
  p_random_erasing: 0.0
  p_autocontrast: 0.0
  p_mixup: 0.0                  # Disabled - was 0.2 in V3
  p_cutmix: 0.0                 # Disabled

# -----------------------------
# MODEL ARCHITECTURE
# -----------------------------
model:
  backbone: "r3d_18"
  pretrained: true
  pretrained_weights: "KINETICS400_V1"
  num_classes: 2
  
  # Regularization - slightly increased
  dropout: 0.4                  # V3 had 0.3
  drop_path: 0.1                # Stochastic depth
  
  # Freezing strategy
  freeze_stages: 1              # Freeze stem + stage1
  freeze_bn: false              # Allow BN to adapt

# -----------------------------
# LOSS FUNCTION
# -----------------------------
loss:
  name: "focal"
  gamma: 2.0                    # Focus on hard examples
  alpha: null                   # Auto-compute from class frequencies
  use_class_weights: true       # Re-enabled (was false in your draft)
  label_smoothing: 0.1          # Soft targets for calibration
  
  # Class weight computation
  class_weight_method: "inverse_sqrt"  # Less aggressive than inverse

# -----------------------------
# OPTIMIZER
# -----------------------------
optim:
  name: "adamw"
  lr: 0.0001                    # Same as V3
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# -----------------------------
# LEARNING RATE SCHEDULER
# -----------------------------
scheduler:
  name: "cosine_warmup"
  warmup_epochs: 3              # Slightly longer warmup
  warmup_start_lr: 1.0e-6
  min_lr: 1.0e-6
  t_max: 30                     # Full cosine cycle

# -----------------------------
# TRAINING LOOP
# -----------------------------
train:
  epochs: 30
  
  # Checkpointing
  metric_key: "val_tail_f1"
  metric_mode: "max"
  save_last: true
  save_best: true
  
  # Gradient handling
  grad_clip: 1.0
  grad_accumulation: 1
  
  # Mixed precision
  use_amp: true
  
  # === OVERFITTING MONITORING ===
  log_train_metrics_every: 10   # Log every N batches
  compute_train_epoch_metrics: true  # Full train set eval each epoch

# -----------------------------
# CALLBACKS
# -----------------------------
callbacks:
  # --- Early Stopping ---
  early_stopping:
    enabled: true
    monitor: "val_tail_f1"
    mode: "max"
    patience: 10
    min_delta: 0.001
    verbose: true
    restore_best_weights: true
  
  # --- Model Checkpoint ---
  model_checkpoint:
    enabled: true
    monitor: "val_tail_f1"
    mode: "max"
    save_best_only: false       # Save best AND periodic
    save_last: true
    save_every_n_epochs: 5      # Periodic checkpoints
    filename_format: "epoch_{epoch:02d}_tailf1_{val_tail_f1:.3f}.pt"
    max_checkpoints: 3          # Keep only top 3
  
  # --- Learning Rate Monitor ---
  lr_monitor:
    enabled: true
    log_momentum: true
  
  # --- Overfitting Detector ---
  overfitting_detector:
    enabled: true
    train_metric: "train_loss"
    val_metric: "val_loss"
    max_gap: 0.3                # Alert if train_loss - val_loss > 0.3
    check_after_epoch: 5        # Start checking after epoch 5
    action: "warn"              # "warn" or "stop"
  
  # --- Gradient Monitor ---
  gradient_monitor:
    enabled: true
    log_every_n_steps: 50
    log_grad_norm: true
    log_grad_histogram: false   # Expensive, disable for speed
    alert_on_nan: true
    alert_on_exploding: true
    exploding_threshold: 100.0
  
  # --- Training Progress ---
  progress_bar:
    enabled: true
    refresh_rate: 10            # Update every 10 batches
    show_metrics: ["loss", "tail_f1", "lr"]
  
  # --- CSV Logger ---
  csv_logger:
    enabled: true
    filename: "training_log.csv"
    log_every_epoch: true
    columns:
      - epoch
      - train_loss
      - val_loss
      - train_tail_f1
      - val_tail_f1
      - val_tail_recall
      - val_tail_precision
      - val_ece
      - lr
      - train_val_gap
  
  # --- TensorBoard Logger ---
  tensorboard:
    enabled: true
    log_dir: "runs/ablation_E02/tensorboard"
    log_graph: false            # Model graph (one-time)
    log_embeddings: false       # Expensive
    log_histograms: false       # Weight histograms
    log_images: true            # Sample predictions
    log_images_every: 5         # Every N epochs
    num_images: 8               # Samples per log
  
  # --- Learning Curve Plotter ---
  learning_curve:
    enabled: true
    plot_every_epoch: true
    save_path: "runs/ablation_E02/learning_curves.png"
    metrics:
      - ["train_loss", "val_loss"]
      - ["train_tail_f1", "val_tail_f1"]
      - ["val_tail_recall", "val_tail_precision"]
  
  # --- Confusion Matrix Logger ---
  confusion_matrix:
    enabled: true
    log_every_n_epochs: 5
    normalize: true
    save_path: "runs/ablation_E02/confusion_matrix_epoch_{epoch}.png"
  
  # --- Timer ---
  timer:
    enabled: true
    log_epoch_time: true
    log_step_time: false
    estimate_remaining: true

# -----------------------------
# VALIDATION
# -----------------------------
validation:
  batch_size: 16
  
  # Multi-clip evaluation (TTA-lite)
  multi_clip: true
  clips_per_event: 5
  aggregation: "mean"           # Average predictions across clips
  
  # Compute detailed metrics
  compute_confusion_matrix: true
  compute_per_class_metrics: true
  compute_calibration: true     # ECE, reliability diagram

# -----------------------------
# TEMPERATURE SCALING (POST-HOC)
# -----------------------------
temperature_scaling:
  enabled: true
  
  # Fit temperature on validation set AFTER training
  fit_on: "val"                 # Use val set to fit T
  optimize_metric: "ece"        # Minimize ECE
  
  # Temperature bounds
  t_min: 0.5
  t_max: 3.0
  t_init: 1.5                   # Initial guess
  
  # Optimization
  method: "lbfgs"               # scipy.optimize
  max_iter: 100

# -----------------------------
# THRESHOLD OPTIMIZATION
# -----------------------------
threshold_optimization:
  enabled: true
  
  # Search range
  thresholds: [0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60]
  
  # Optimize for welfare monitoring (prioritize recall)
  primary_metric: "tail_f1"
  secondary_metric: "tail_recall"
  min_precision: 0.30           # Floor for precision

# -----------------------------
# OUTPUT & LOGGING
# -----------------------------
output:
  dir: "runs/ablation_E02"
  save_predictions: true
  save_logits: true             # For temperature scaling
  save_embeddings: false
  
  # Logging
  log_to_tensorboard: true
  log_to_csv: true
  
  # Overfitting diagnostics
  plot_learning_curves: true
  plot_train_val_gap: true

# -----------------------------
# EVALUATION PROTOCOL
# -----------------------------
evaluation:
  # Standard metrics
  metrics:
    - accuracy
    - balanced_accuracy
    - macro_f1
    - per_class_f1
    - per_class_precision
    - per_class_recall
    - mcc
    - pr_auc
    - roc_auc
    - ece
  
  # Bootstrap CI for tail metrics (small sample)
  bootstrap:
    enabled: true
    n_iterations: 1000
    confidence_level: 0.95
    metrics: ["tail_f1", "tail_precision", "tail_recall"]
  
  # Final test evaluation
  test:
    manifest: "data/manifests/test.csv"  # If available
    apply_temperature: true
    apply_threshold: true