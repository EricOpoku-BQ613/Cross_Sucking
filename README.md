# Cross-Sucking Detection Pipeline

**Role-Aware Interaction Tubes for Livestock Welfare Monitoring**

---

## ğŸ¯ Overview

Automated detection of cross-sucking behavior in dairy calves using computer vision, with focus on:
- **WHO**: Identifying initiator vs receiver roles
- **WHAT**: Classifying target body region (ear/tail/teat/other)
- **WHEN**: Precise temporal event localization

### Key Innovation: Role-Conditioned Graph Attention Network (RCGAN)
Unlike existing approaches that treat both animals symmetrically, RCGAN explicitly models the **asymmetric dynamics** between initiator and receiverâ€”leveraging the behavioral insight that initiators control event termination.

---

## ğŸ“ Project Structure

```
cross_sucking/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ base.yaml              # Shared settings
â”‚   â”œâ”€â”€ paths.yaml             # Data paths (gitignored)
â”‚   â”œâ”€â”€ pretrain.yaml          # SSL pretraining
â”‚   â”œâ”€â”€ detect.yaml            # Object detection
â”‚   â”œâ”€â”€ track.yaml             # Multi-object tracking
â”‚   â”œâ”€â”€ tubes.yaml             # Tube generation
â”‚   â”œâ”€â”€ model.yaml             # RCGAN model
â”‚   â””â”€â”€ eval.yaml              # Evaluation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Symlinks to original data
â”‚   â”œâ”€â”€ annotations/           # Labels + mapping rules
â”‚   â”œâ”€â”€ manifests/             # Generated manifests
â”‚   â”œâ”€â”€ interim/               # Intermediate artifacts
â”‚   â””â”€â”€ processed/             # Training-ready data
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/                   # Command-line tools
â”‚   â”œâ”€â”€ datasets/              # PyTorch datasets
â”‚   â”œâ”€â”€ models/                # Model architectures
â”‚   â”œâ”€â”€ eval/                  # Evaluation metrics
â”‚   â”œâ”€â”€ utils/                 # Utilities
â”‚   â””â”€â”€ viz/                   # Visualization
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”œâ”€â”€ runs/                      # Experiment outputs
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ tests/                     # Unit tests
â””â”€â”€ scripts/                   # Shell scripts
```

---

## ğŸ“Š Data Overview

### Labeled Data
| Group | Day | Cameras | Events |
|-------|-----|---------|--------|
| 1 | 1 | 7, 8, 9, 10 | 316 |
| 1 | 4 | 7, 8, 9, 10 | 407 |
| 2 | 4 | 10, 11, 12, 116 | 237 |
| 3 | 4 | 8, 12, 14, 16 | 333+ |

### Unlabeled Data (~1,680 hours)
- Groups 2-6, Week 2
- 4 cameras per group
- Used for SSL pretraining

### Behavior Distribution
- **Ear**: 87.1% (1,703 events)
- **Tail**: 11.7% (228 events)
- **Teat**: 0.6% (12 events) â† Rare but critical

---

## ğŸš€ Quick Start

### 1. Setup
```bash
# Clone repository
git clone https://github.com/your-username/cross_sucking.git
cd cross_sucking

# Create environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install package
pip install -e ".[all]"
```

### 2. Configure Paths
```bash
# Copy and edit paths config
cp configs/paths.yaml.example configs/paths.yaml
# Edit paths.yaml with your data locations
```

### 3. Build Manifests
```bash
# Scan all videos and create manifest
cs-manifest build --config configs/paths.yaml

# Clean and normalize annotations
cs-clean data/annotations/interactions.xlsx
```

### 4. Run Pipeline
```bash
# Stage 1: SSL Pretraining (optional but recommended)
python -m src.cli.pretrain_ssl --config configs/pretrain.yaml

# Stage 2: Detection
cs-detect --config configs/detect.yaml

# Stage 3: Tracking
cs-track --config configs/track.yaml

# Stage 4: Build Tubes
cs-tubes --config configs/tubes.yaml

# Stage 5: Train Interaction Model
cs-train --config configs/model.yaml

# Stage 6: Evaluate
cs-eval --config configs/eval.yaml
```

---

## ğŸ“ˆ Pipeline Stages

```
Stage 0: Data Foundation (Week 1-2)
â”œâ”€â”€ Build video manifest
â”œâ”€â”€ Clean annotations  
â”œâ”€â”€ Link events to videos
â””â”€â”€ Create train/val/test splits

Stage 1: Foundation Encoder (Week 3-4)
â”œâ”€â”€ SSL pretrain on unlabeled data
â”œâ”€â”€ Fine-tune on labeled events
â””â”€â”€ Extract embeddings

Stage 2: Detection + Tracking (Week 5-6)
â”œâ”€â”€ YOLOv8 calf detection
â”œâ”€â”€ ByteTrack multi-object tracking
â””â”€â”€ Generate tracklets

Stage 3: Interaction Tubes (Week 7-8)
â”œâ”€â”€ Pairwise tube proposals
â”œâ”€â”€ RCGAN training
â””â”€â”€ Role + target classification

Stage 4: Temporal Refinement (Week 9)
â”œâ”€â”€ Boundary-aware scoring
â””â”€â”€ Temporal NMS

Stage 5: Uncertainty + Active Learning (Week 10)
â”œâ”€â”€ MC-Dropout calibration
â”œâ”€â”€ Abstain policy
â””â”€â”€ Sample selection for annotation
```

---

## ğŸ¯ Evaluation Metrics

| Metric | Description | Target |
|--------|-------------|--------|
| mAP@0.3 | Temporal detection (loose) | 0.55-0.65 |
| mAP@0.5 | Temporal detection (strict) | 0.45-0.55 |
| Role Accuracy | Initiator vs receiver | 0.75-0.85 |
| Target F1 | Ear/tail/teat/other | 0.50-0.60 |
| Teat F1 | Rare class | 0.35-0.45 |

---

## ğŸ› ï¸ CLI Commands

| Command | Description |
|---------|-------------|
| `cs-manifest build` | Build video manifest |
| `cs-manifest verify` | Verify manifest integrity |
| `cs-clean` | Clean annotations |
| `cs-detect` | Run object detection |
| `cs-track` | Run multi-object tracking |
| `cs-tubes` | Generate interaction tubes |
| `cs-train` | Train interaction model |
| `cs-eval` | Evaluate model |

---

## ğŸ“š References

- [Agriculture-Vision Workshop](https://www.agriculture-vision.com/)
- [VideoMAE](https://arxiv.org/abs/2203.12602)
- [ByteTrack](https://arxiv.org/abs/2110.06864)
- [ActionFormer](https://arxiv.org/abs/2202.07925)

---

## ğŸ“ License

MIT License

---

## ğŸ‘¥ Contributors

- Your Name
